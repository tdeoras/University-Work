{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woZuGLfokoCx"
   },
   "source": [
    "# CSCI-P556\n",
    "# Assignment 1\n",
    "# Due date: Friday, February 15, 11:59PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5d1GxwAzsRfB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCxVSEY_ku5i"
   },
   "source": [
    "## Question 1 (10 points)\n",
    "\n",
    "Implement linear regression using ordinary least squares (closed-form solution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQevk_umkdh3"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from numpy import array, dot, transpose\n",
    "from numpy.linalg import inv\n",
    "\n",
    "#X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "#y = np.array([5,6,7,8])\n",
    "def linear_regression_ols(X, y):  \n",
    "  # Make sure that you return the weights in a np.array, \n",
    "  # other data types will cause our grading script to crash \n",
    "  w = dot(dot((inv(dot((transpose((np.column_stack(((np.ones(len(X))),X))))), (np.column_stack(((np.ones(len(X))),X)))))), (transpose((np.column_stack(((np.ones(len(X))),X)))))), y)\n",
    "  return w\n",
    "\n",
    "#X = np.array([[3, 4], [4, 5], [9, 3]])\n",
    "#y = np.array([3,4,5])\n",
    "#print (linear_regression_ols(X, y))\n",
    "\n",
    "#reg = linear_model.LinearRegression()\n",
    "#reg.fit([[3, 4], [4, 5], [9, 3]], [3, 4, 5])\n",
    "#print (reg.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_CofGkGmgR6"
   },
   "source": [
    "## Question 2 (40 points)\n",
    "\n",
    "Implement linear regression using gradient descent. A boolean parameter named *regularization* has been included in the function definition. If regularization=True, then the linear regression will be computed using L2 regularization.\n",
    "\n",
    "![L2 regularization](https://cdn-images-1.medium.com/max/1200/1*jgWOhDiGjVp-NCSPa5abmg.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCS581-dm3PV"
   },
   "outputs": [],
   "source": [
    "def linear_regression_gd(X, y, theta,learning_rate,num_iters,regularization,lambda1):\n",
    "    \n",
    "    \n",
    "    if regularization == True:\n",
    "        \n",
    "        m = len(y)\n",
    "        for i in range(num_iters):\n",
    "            prediction = np.dot(X,theta)\n",
    "            theta = theta - (1/m) * learning_rate * (X.T.dot((prediction - y))) + (lambda1/m) * np.sum(theta)\n",
    "        return theta    \n",
    "        \n",
    "        \n",
    "    else:    \n",
    "        m = len(y)\n",
    "        for i in range(num_iters):\n",
    "            prediction = np.dot(X,theta)\n",
    "            theta = theta - (1/m) * learning_rate * (X.T.dot((prediction - y)))\n",
    "        return theta\n",
    "    \n",
    "#X = np.array([[3, 4], [4, 5], [9, 3]])\n",
    "#X = ((np.column_stack(((np.ones(len(X))),X))))\n",
    "#y = np.array([3,4,5])\n",
    "#learning_rate = 0.00001\n",
    "#num_iters = 100\n",
    "#theta = np.ones(len(X[1]))\n",
    "#lambda1 = 1\n",
    "#linear_regression_gd(X, y, theta, learning_rate, num_iters, False, lambda1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VV6PVnE0n4eb"
   },
   "source": [
    "## Question 3 (20 points)\n",
    "\n",
    "- Apply your linear regression OLS, gradient descent without regularization, and gradient descent with regularization functions to [Sci-Kit Learn's diabetes dataset](https://www.programcreek.com/python/example/85913/sklearn.datasets.load_diabetes). Additionally, apply [Sci-Kit Learn's Linear Regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- Calculate the amount of time it took for each of the functions to execute with the code that we have included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFVwHrYfpKbI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CSV from URL using NumPy\n",
    "from numpy import loadtxt\n",
    "from urllib.request import urlopen\n",
    "url = 'https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt'\n",
    "raw_data = urlopen(url)\n",
    "dataset = loadtxt(raw_data, delimiter=\"\\t\", skiprows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlwnxRsarM0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time for OLS: -0.0045069999999984844\n",
      "Total execution time for gradient descent without regularization: -0.009606000000001558\n",
      "Total execution time for gradient descent with regularization: -0.015000000000000568\n",
      "Total execution time for Sci-Kit Learn's Linear Regression: -0.008120000000001681\n"
     ]
    }
   ],
   "source": [
    "# load variables in this area\n",
    "#X0 = np.ones(dataset[:,0:-1].shape[0])\n",
    "X = dataset[:,0:-1]\n",
    "y = dataset[:,10]\n",
    "\n",
    "start_time = time.process_time()\n",
    "linear_regression_ols(X, y)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for OLS: \" + str(start_time-end_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "learning_rate = 0.00001\n",
    "num_iters = 100\n",
    "theta = np.ones(len(X[1]))\n",
    "lambda1 = 1\n",
    "linear_regression_gd(X, y, theta, learning_rate, num_iters, False, lambda1)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for gradient descent without regularization: \" + str(start_time-end_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "learning_rate = 0.00001\n",
    "num_iters = 100\n",
    "theta = np.ones(len(X[1]))\n",
    "lambda1 = 1\n",
    "linear_regression_gd(X, y, theta, learning_rate, num_iters, True, lambda1)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for gradient descent with regularization: \" + str(start_time-end_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X,y)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for Sci-Kit Learn's Linear Regression: \" + str(start_time-end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7p2lIOzc5T-"
   },
   "source": [
    "## Question 4 (20 points)\n",
    "\n",
    "Normalize the appropriate variables in the dataset and re-do Question 3 using this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ON8HYSCjc2Hv"
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfN0z1ZjdcHk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time for OLS: -0.002691000000000443\n",
      "Total execution time for gradient descent without regularization: -0.012724999999997877\n",
      "Total execution time for gradient descent with regularization: -0.012959000000000387\n",
      "Total execution time for Sci-Kit Learn's Linear Regression: -0.0037300000000008993\n"
     ]
    }
   ],
   "source": [
    "X = normalize_dataset(X)\n",
    "\n",
    "start_time = time.process_time()\n",
    "linear_regression_ols(X, y)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for OLS: \" + str(start_time-end_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "learning_rate = 0.00001\n",
    "num_iters = 100\n",
    "theta = np.ones(len(X[1]))\n",
    "lambda1 = 1\n",
    "linear_regression_gd(X, y, theta, learning_rate, num_iters, False, lambda1)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for gradient descent without regularization: \" + str(start_time-end_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "learning_rate = 0.00001\n",
    "num_iters = 100\n",
    "theta = np.ones(len(X[1]))\n",
    "lambda1 = 1\n",
    "linear_regression_gd(X, y, theta, learning_rate, num_iters, True, lambda1)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for gradient descent with regularization: \" + str(start_time-end_time))\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X,y)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for Sci-Kit Learn's Linear Regression: \" + str(start_time-end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8gPX8LruFKw"
   },
   "source": [
    "## Question 5 (10 points, 2 points per quesiton)\n",
    "\n",
    "1. Did you notice any difference between the normalize and non-normalized versions in questions 3 and 4? Explain your answer.\n",
    "2. Which is the linear regressions is faster? Why is it faster?\n",
    "3. Why don't we train all the machine learning models using that technique?\n",
    "4. Describe in your own words at least two regularization methods\n",
    "5. What would happen if you use a regularization parameter value that is too low or too high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJevEyH8KPWZ"
   },
   "source": [
    "Write your answers here:\n",
    "\n",
    "1. Normalization caused the accuracy to increase and caused coeffs to change.Normalized version also took more time.(Check code below for proof)\n",
    "\n",
    "2. For this dataset OLS performed the best. But for scenarios where the number of features is large Gradient Performs better. A small feature space casued OLS to perform better.\n",
    "\n",
    "3. Data variations cause different regression techniques to perform differently. Grandient decent performs better when the number of features is large while OLS performs better when feature space is small.\n",
    "\n",
    "4. Regularization penalizes models based on their complexity by favoring simpler models.\n",
    "\n",
    "   Ridge Regularization - It causes the weights of features to decrease making the model less complex and avoids              overfitting.\n",
    "   \n",
    "   Lasso Regularization - Causes causes features with less weights to eqauate to 0. Hence ignoring all the features           not important for classification\n",
    "\n",
    "5. If regularization parameter is high - underfitting and if regularization parameter is low - overfitting.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time for no Normalization: -0.0014419999999990551\n",
      "Accuracy without Normalization\n",
      "0.5177484222203499\n",
      "Total execution time for Normalization: -0.0026810000000008216\n",
      "Accuracy with Normalization\n",
      "0.5177484222203498\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from numpy import loadtxt\n",
    "from urllib.request import urlopen\n",
    "url = 'https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt'\n",
    "raw_data = urlopen(url)\n",
    "dataset = loadtxt(raw_data, delimiter=\"\\t\", skiprows = 1)\n",
    "x_test = dataset[:,0:-1]\n",
    "y_test = dataset[:,10]\n",
    "\n",
    "regressor = LinearRegression()\n",
    "start_time = time.process_time()\n",
    "regressor.fit(x_test,y_test)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for no Normalization: \" + str(start_time-end_time))\n",
    "accuracy = regressor.score(x_test,y_test)\n",
    "print (\"Accuracy without Normalization\")\n",
    "print (accuracy)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "x_test = normalize_dataset(x_test)\n",
    "start_time = time.process_time()\n",
    "regressor.fit(x_test,y_test)\n",
    "end_time = time.process_time()\n",
    "print(\"Total execution time for Normalization: \" + str(start_time-end_time))\n",
    "accuracy = regressor.score(x_test,y_test)\n",
    "print (\"Accuracy with Normalization\")\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "P556-Assignment1-S19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
